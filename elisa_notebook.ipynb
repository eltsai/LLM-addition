{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models Use Trigonometry to Do Addition\n",
    "\n",
    "## **Objective**\n",
    "The goal of this study is to investigate how transformers utilize **hidden state representations** to perform fundamental mathematical operations such as **addition**. Specifically, the study explores:\n",
    "1. **How numbers are represented within the residual stream** across multiple layers.\n",
    "2. **How attention and MLP mechanisms interact** to modify and propagate numerical representations.\n",
    "3. **What underlying algorithms transformers learn** to perform arithmetic operations.\n",
    "\n",
    "\n",
    "**[Terminology]** Helix: A helix is a three-dimensional curve that winds around a central axis while simultaneously moving along it.\n",
    "\n",
    "**[Terminology]** General Helix: A curve is called a general helix if its tangent makes a constant angle with a fixed line in space.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Helical Number Representations\n",
    "- LLMs encode numbers as a **generalized helix** \n",
    "\n",
    "- The periodicity aligns with **Fourier components** (T = [2, 5, 10, 100]):\n",
    "  - T = 2: Evenness\n",
    "  - T = 5, 10: Base-10 structure\n",
    "  - T = 100: Larger-scale periodicity\n",
    "- This helical representation is **causally relevant** for:\n",
    "  - Addition\n",
    "  - Subtraction\n",
    "  - Integer division\n",
    "  - Multiplication\n",
    "  - Modular arithmetic\n",
    "\n",
    "### 2. Clock Algorithm for Addition\n",
    "- LLMs perform addition by **manipulating these helices**, extending Nanda et al.'s \"Clock\" algorithm.\n",
    "- **Mechanism:**\n",
    "  1. **Embedding:** Numbers are encoded as helices.\n",
    "  2. **Attention heads (Layers 9-14):** Move the helices of a and b to the last token.\n",
    "  3. **MLPs (Layers 14-18):** Construct the a + b helix by manipulating a and b helices.\n",
    "  4. **MLPs (Layers 19-27):** Read from the a + b helix to produce the final logits.\n",
    "\n",
    "### 3. Empirical Validation\n",
    "- **Activation patching** shows that the helical representation is **strongly implicated** in addition.\n",
    "- Helical representations **outperform PCA-based models** in preserving numerical structure.\n",
    "\n",
    "### 4. Neuron-Level Investigation\n",
    "- **Neuron preactivations** show periodic patterns aligned with the helical model.\n",
    "- The authors hypothesize LLMs use **trigonometric identities** (e.g., `cos(a + b) = cos(a) cos(b) - sin(a) sin(b)`) to construct the sum helix.\n",
    "\n",
    "### 5. Limitations & Future Directions\n",
    "- The exact mechanism for transforming helices **remains unclear**.\n",
    "- Results may not generalize to **models that tokenize numbers digit-by-digit**.\n",
    "- LLMs may favor helices over linear representations **for robustness to noise**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    " <img src=\"fig1.jpg\" width=\"600\"/>\n",
    "\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "### **Background**\n",
    "Autoregressive transformers are widely used in language models to process sequential data by predicting the next token in a given sequence. These models take as input a sequence of tokens $x_0, ..., x_n$ and generate probability distributions over possible next tokens $x_{n+1}$, enabling tasks such as text generation, mathematical reasoning, and code completion.\n",
    "\n",
    "### **Formal Definition**\n",
    "Given a sequence of tokens $x_0, x_1, ..., x_n$, an autoregressive transformer processes each token through a **residual stream** comprising multiple layers of computation. Each layer consists of two primary components: **multi-head self-attention** and **multi-layer perceptrons (MLPs)**. The hidden state representation at layer $l$ for token $i$ is computed as:\n",
    "\n",
    "$$\n",
    "h^l_i = h^{l-1}_i + a^l_i + m^l_i\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $h^l_i$ represents the hidden state at layer $l$ for token $i$,\n",
    "- $a^l_i$ is the output of the **attention mechanism**, computed as:\n",
    "\n",
    "  $$\n",
    "  a^l_i = \\text{attn}^l(h^{l-1}_1, h^{l-1}_2, ..., h^{l-1}_i)\n",
    "  $$\n",
    "\n",
    "- $m^l_i$ is the output of the **MLP component**, computed as:\n",
    "\n",
    "  $$\n",
    "  m^l_i = \\text{MLP}^l(a^l_i + h^{l-1}_i)\n",
    "  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eltsai/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from nnsight import LanguageModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model# Set the token as an environment variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "full_model_name = 'EleutherAI/gpt-j-6B'#'meta-llama/Llama-3.1-8B'# #'EleutherAI/pythia-6.9b'## # #'google/gemma-2-9b'#'\n",
    "MODEL_NAME = full_model_name.split('/')[-1]\n",
    "model = LanguageModel(full_model_name, device_map=device, torch_dtype=torch.bfloat16, dispatch=True)\n",
    "remote = False\n",
    "NLAYERS = model.config.num_hidden_layers\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJForCausalLM(\n",
       "  (transformer): GPTJModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-27): 28 x GPTJBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTJAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): GPTJMLP(\n",
       "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"transformers.png\" alt=\"Transformers architecture\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate addition evaluation data\n",
    "\n",
    "**Data Generation for Addition Evaluation**\n",
    "In this section, we generate evaluation data for testing the model's addition capabilities.\n",
    "\n",
    "We create a dataset of addition problems with numbers ranging from 0 to 99.\n",
    "For each pair of numbers (a,b), we:\n",
    "  1. Generate a prompt string asking for the sum\n",
    "  2. Tokenize the prompt using the model's tokenizer\n",
    "  3. Calculate the correct answer (a+b)\n",
    "  4. Tokenize the answer\n",
    "  5. Store all this information in a pandas DataFrame\n",
    "The data is then saved to a pickle file for later evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 134.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>q_string</th>\n",
       "      <th>q_tok</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Output ONLY a number.\\n0+0=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 15, 10, 15,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Output ONLY a number.\\n0+1=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 15, 10, 16,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Output ONLY a number.\\n0+2=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 15, 10, 17,...</td>\n",
       "      <td>2</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Output ONLY a number.\\n0+3=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 15, 10, 18,...</td>\n",
       "      <td>3</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Output ONLY a number.\\n0+4=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 15, 10, 19,...</td>\n",
       "      <td>4</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "      <td>Output ONLY a number.\\n99+95=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2079, 10, 3...</td>\n",
       "      <td>194</td>\n",
       "      <td>[22913]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>99</td>\n",
       "      <td>96</td>\n",
       "      <td>Output ONLY a number.\\n99+96=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2079, 10, 4...</td>\n",
       "      <td>195</td>\n",
       "      <td>[22186]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>99</td>\n",
       "      <td>97</td>\n",
       "      <td>Output ONLY a number.\\n99+97=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2079, 10, 5...</td>\n",
       "      <td>196</td>\n",
       "      <td>[25272]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>99</td>\n",
       "      <td>98</td>\n",
       "      <td>Output ONLY a number.\\n99+98=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2079, 10, 4...</td>\n",
       "      <td>197</td>\n",
       "      <td>[24991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>Output ONLY a number.\\n99+99=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2079, 10, 2...</td>\n",
       "      <td>198</td>\n",
       "      <td>[22337]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       a   b                       q_string  \\\n",
       "0      0   0    Output ONLY a number.\\n0+0=   \n",
       "1      0   1    Output ONLY a number.\\n0+1=   \n",
       "2      0   2    Output ONLY a number.\\n0+2=   \n",
       "3      0   3    Output ONLY a number.\\n0+3=   \n",
       "4      0   4    Output ONLY a number.\\n0+4=   \n",
       "...   ..  ..                            ...   \n",
       "9995  99  95  Output ONLY a number.\\n99+95=   \n",
       "9996  99  96  Output ONLY a number.\\n99+96=   \n",
       "9997  99  97  Output ONLY a number.\\n99+97=   \n",
       "9998  99  98  Output ONLY a number.\\n99+98=   \n",
       "9999  99  99  Output ONLY a number.\\n99+99=   \n",
       "\n",
       "                                                  q_tok  answer answer_tok  \n",
       "0     [26410, 22224, 257, 1271, 13, 198, 15, 10, 15,...       0       [15]  \n",
       "1     [26410, 22224, 257, 1271, 13, 198, 15, 10, 16,...       1       [16]  \n",
       "2     [26410, 22224, 257, 1271, 13, 198, 15, 10, 17,...       2       [17]  \n",
       "3     [26410, 22224, 257, 1271, 13, 198, 15, 10, 18,...       3       [18]  \n",
       "4     [26410, 22224, 257, 1271, 13, 198, 15, 10, 19,...       4       [19]  \n",
       "...                                                 ...     ...        ...  \n",
       "9995  [26410, 22224, 257, 1271, 13, 198, 2079, 10, 3...     194    [22913]  \n",
       "9996  [26410, 22224, 257, 1271, 13, 198, 2079, 10, 4...     195    [22186]  \n",
       "9997  [26410, 22224, 257, 1271, 13, 198, 2079, 10, 5...     196    [25272]  \n",
       "9998  [26410, 22224, 257, 1271, 13, 198, 2079, 10, 4...     197    [24991]  \n",
       "9999  [26410, 22224, 257, 1271, 13, 198, 2079, 10, 2...     198    [22337]  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "def gen_math(mina = 0, maxa = 99):\n",
    "    data = []\n",
    "    with torch.no_grad():\n",
    "        for a in tqdm(range(mina, maxa + 1)):\n",
    "            for b in range(mina, maxa + 1):\n",
    "                if MODEL_NAME == 'gpt-j-6B':\n",
    "                    q_string = f'Output ONLY a number.\\n{a}+{b}='\n",
    "                elif MODEL_NAME == 'Llama-3.1-8B':\n",
    "                    q_string = f'The following is a correct addition problem. \\n{a}+{b}='\n",
    "                    #q_string = f'The following is a correction addition problem. {a}+0='\n",
    "                elif MODEL_NAME == 'pythia-6.9b':\n",
    "                    q_string = f'Output ONLY a number. {a}+{b}='\n",
    "                q_toks = model.tokenizer(q_string)['input_ids']\n",
    "                answer = a+b\n",
    "                answer_tok = model.tokenizer(f'{answer}')['input_ids']\n",
    "                if MODEL_NAME == 'Llama-3.1-8B':\n",
    "                    answer_tok = [answer_tok[-1]]\n",
    "                #print(answer_tok)\n",
    "                data.append({\n",
    "                    'a': a,\n",
    "                    'b': b,\n",
    "                    'q_string': q_string,\n",
    "                    'q_tok': q_toks,\n",
    "                    'answer': answer,\n",
    "                    'answer_tok': answer_tok\n",
    "                })\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_pickle(f'data_addition/gen_math/data_addition_{mina}_{maxa}_{MODEL_NAME}.pkl')\n",
    "    return df\n",
    "\n",
    "\n",
    "#evaluate_math(mina = 0, maxa = 500)\n",
    "gen_math(mina = 0, maxa = 99)\n",
    "#data = get_math_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_math(mina = 0, maxa = 99, verbose = False, batch_size = 100):\n",
    "    df = pd.read_pickle(f'data_addition/gen_math/data_addition_{mina}_{maxa}_{MODEL_NAME}.pkl')\n",
    "    #df = df.sample(n=15, random_state=42)\n",
    "    corrects = []\n",
    "    with torch.no_grad():\n",
    "        # Process rows in batches\n",
    "        bar = tqdm(range(0, len(df), batch_size))\n",
    "        for step in bar:\n",
    "            batch_df = df.iloc[step:min(step + batch_size, len(df))]\n",
    "            batch_toks = torch.stack([torch.tensor(x) for x in batch_df['q_tok'].values]).to(device)\n",
    "            # Combine into single batch tensor\n",
    "            answer_toks = torch.stack([torch.tensor(x)[0] for x in batch_df['answer_tok'].values])\n",
    "            # Get model outputs for batch\n",
    "            with model.trace() as tracer:\n",
    "                with tracer.invoke(batch_toks) as invoker:\n",
    "                    pass\n",
    "                output = model.output.save()\n",
    "            \n",
    "            # Get predictions for batch\n",
    "            logits = output.logits[:,-1].cpu()\n",
    "            #print(model.tokenizer.batch_decode(output.logits[0].argmax(dim=-1)))\n",
    "            model_answers = logits.argmax(dim=-1)\n",
    "            correct = (model_answers == answer_toks).float()\n",
    "            incorrect_mask = correct == 0\n",
    "            if incorrect_mask.any() and verbose:\n",
    "                print(\"Incorrect answers:\")\n",
    "                print(\"Expected:\", model.tokenizer.batch_decode(answer_toks[incorrect_mask].unsqueeze(-1)))\n",
    "                print(\"Got:\", model.tokenizer.batch_decode(model_answers[incorrect_mask].unsqueeze(-1)))\n",
    "            corrects.extend(list(correct))\n",
    "            bar.set_postfix({'%': np.mean(corrects)})\n",
    "            \n",
    "    df['correct'] = corrects\n",
    "    df.to_pickle(f'data_addition/gen_math/data_addition_correct_{mina}_{maxa}_{MODEL_NAME}.pkl')\n",
    "    return np.mean(corrects)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 100/100 [00:14<00:00,  6.70it/s, %=0.806]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(0.8061)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_math(mina = 0, maxa = 99, verbose = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Generated Data\n",
    "\n",
    "In this section, we load and process the generated data from our model's addition task. \n",
    "\n",
    "1. We sample a subset of those examples, and map the numerical answers to their corresponding tokens. \n",
    "2. We then use these functions to load a sample of 500 correctly predicted addition problems and prepare it for further analysis. \n",
    "3. The data includes the operands (a and b), the question string, tokenized inputs/outputs, and correctness flags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_df(mina = 0, maxa = 99):\n",
    "    df = pd.read_pickle(f'data_addition/gen_math/data_addition_correct_{mina}_{maxa}_{MODEL_NAME}.pkl')\n",
    "    return df[df['correct'] == 1]\n",
    "\n",
    "def get_df_sample(mina = 0, maxa = 99, num_sample = 500, run = False):\n",
    "    if run:\n",
    "        df = get_correct_df(mina, maxa)\n",
    "        df = df.sample(n=num_sample, random_state=42).reset_index(drop=True)\n",
    "        # Save sampled dataframe\n",
    "        save_path = f'data_addition/gen_math/data_addition_correct_sample_{mina}_{maxa}_{MODEL_NAME}.pkl'\n",
    "        df.to_pickle(save_path)\n",
    "    return pd.read_pickle(f'data_addition/gen_math/data_addition_correct_sample_{mina}_{maxa}_{MODEL_NAME}.pkl')\n",
    "\n",
    "def get_output_tokens(mina = 0, maxa = 99):\n",
    "    # answers range from 0, 99 * 2 \n",
    "    avals, toks = [], []\n",
    "    for a in range(mina, 2 * maxa + 1):\n",
    "        atok = model.tokenizer(f'{a}')['input_ids']\n",
    "        avals.append(a)\n",
    "        toks.append(atok[0])\n",
    "    return avals, toks\n",
    "\n",
    "a, toks = get_output_tokens()\n",
    "df = get_df_sample(run = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>q_string</th>\n",
       "      <th>q_tok</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_tok</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>Output ONLY a number.\\n9+51=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 24, 10, 434...</td>\n",
       "      <td>60</td>\n",
       "      <td>[1899]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>Output ONLY a number.\\n25+39=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 1495, 10, 2...</td>\n",
       "      <td>64</td>\n",
       "      <td>[2414]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>Output ONLY a number.\\n45+49=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2231, 10, 2...</td>\n",
       "      <td>94</td>\n",
       "      <td>[5824]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>Output ONLY a number.\\n56+54=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 3980, 10, 4...</td>\n",
       "      <td>110</td>\n",
       "      <td>[11442]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>Output ONLY a number.\\n77+15=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 3324, 10, 1...</td>\n",
       "      <td>92</td>\n",
       "      <td>[5892]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Output ONLY a number.\\n4+4=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 19, 10, 19,...</td>\n",
       "      <td>8</td>\n",
       "      <td>[23]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "      <td>Output ONLY a number.\\n75+15=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2425, 10, 1...</td>\n",
       "      <td>90</td>\n",
       "      <td>[3829]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>Output ONLY a number.\\n74+11=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 4524, 10, 1...</td>\n",
       "      <td>85</td>\n",
       "      <td>[5332]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>Output ONLY a number.\\n62+5=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 5237, 10, 2...</td>\n",
       "      <td>67</td>\n",
       "      <td>[3134]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>28</td>\n",
       "      <td>54</td>\n",
       "      <td>Output ONLY a number.\\n28+54=</td>\n",
       "      <td>[26410, 22224, 257, 1271, 13, 198, 2078, 10, 4...</td>\n",
       "      <td>82</td>\n",
       "      <td>[6469]</td>\n",
       "      <td>tensor(1.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a   b                       q_string  \\\n",
       "0     9  51   Output ONLY a number.\\n9+51=   \n",
       "1    25  39  Output ONLY a number.\\n25+39=   \n",
       "2    45  49  Output ONLY a number.\\n45+49=   \n",
       "3    56  54  Output ONLY a number.\\n56+54=   \n",
       "4    77  15  Output ONLY a number.\\n77+15=   \n",
       "..   ..  ..                            ...   \n",
       "495   4   4    Output ONLY a number.\\n4+4=   \n",
       "496  75  15  Output ONLY a number.\\n75+15=   \n",
       "497  74  11  Output ONLY a number.\\n74+11=   \n",
       "498  62   5   Output ONLY a number.\\n62+5=   \n",
       "499  28  54  Output ONLY a number.\\n28+54=   \n",
       "\n",
       "                                                 q_tok  answer answer_tok  \\\n",
       "0    [26410, 22224, 257, 1271, 13, 198, 24, 10, 434...      60     [1899]   \n",
       "1    [26410, 22224, 257, 1271, 13, 198, 1495, 10, 2...      64     [2414]   \n",
       "2    [26410, 22224, 257, 1271, 13, 198, 2231, 10, 2...      94     [5824]   \n",
       "3    [26410, 22224, 257, 1271, 13, 198, 3980, 10, 4...     110    [11442]   \n",
       "4    [26410, 22224, 257, 1271, 13, 198, 3324, 10, 1...      92     [5892]   \n",
       "..                                                 ...     ...        ...   \n",
       "495  [26410, 22224, 257, 1271, 13, 198, 19, 10, 19,...       8       [23]   \n",
       "496  [26410, 22224, 257, 1271, 13, 198, 2425, 10, 1...      90     [3829]   \n",
       "497  [26410, 22224, 257, 1271, 13, 198, 4524, 10, 1...      85     [5332]   \n",
       "498  [26410, 22224, 257, 1271, 13, 198, 5237, 10, 2...      67     [3134]   \n",
       "499  [26410, 22224, 257, 1271, 13, 198, 2078, 10, 4...      82     [6469]   \n",
       "\n",
       "        correct  \n",
       "0    tensor(1.)  \n",
       "1    tensor(1.)  \n",
       "2    tensor(1.)  \n",
       "3    tensor(1.)  \n",
       "4    tensor(1.)  \n",
       "..          ...  \n",
       "495  tensor(1.)  \n",
       "496  tensor(1.)  \n",
       "497  tensor(1.)  \n",
       "498  tensor(1.)  \n",
       "499  tensor(1.)  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANS_SEQPOS_DICT = {'a': -4, 'b':-2, 'a+b':-1}\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00,  8.86it/s]\n"
     ]
    }
   ],
   "source": [
    "target = 'a' #'a'\n",
    "mina = 0\n",
    "maxa = 99\n",
    "batch_size = 80\n",
    "sample = True\n",
    "\n",
    "ANS_SEQPOS = ANS_SEQPOS_DICT[target]\n",
    "\n",
    "a = torch.tensor(df['a'].values)\n",
    "b = torch.tensor(df['b'].values)\n",
    "a_b = a+b\n",
    "hss = []\n",
    "with torch.no_grad():\n",
    "    for step in tqdm(range(0, len(df), batch_size)):\n",
    "        batch_df = df.iloc[step:min(step + batch_size, len(df))]\n",
    "        tokens = torch.stack([torch.tensor(x) for x in batch_df['q_tok'].values]).to(device)\n",
    "        layer_hss = []\n",
    "        with model.trace(validate=False,remote=remote) as tracer:\n",
    "            with tracer.invoke(tokens, scan=False):\n",
    "                for layer in range(NLAYERS):\n",
    "                    if MODEL_NAME == 'Llama-3.1-8B':\n",
    "                        hs = model.model.layers[layer].input\n",
    "                    elif MODEL_NAME == 'gpt-j-6B':\n",
    "                        hs = model.transformer.h[layer].inputs[1]['hidden_states']\n",
    "                    elif MODEL_NAME == 'pythia-6.9b':\n",
    "                        hs = model.gpt_neox.layers[layer].input\n",
    "                    layer_hss.append(hs[:,ANS_SEQPOS].save())\n",
    "        layer_hss = [layer_hs.detach().cpu() for layer_hs in layer_hss] # gets hs on top of ans_token\n",
    "        layer_hss = torch.stack(layer_hss, dim=1) # stack along new layer dimension\n",
    "        hss.append(layer_hss)\n",
    "hss = torch.cat(hss, dim=0) # concat along batch dimension\n",
    "# Save nums and hss to file with descriptive name\n",
    "# Use _FULL in filename if using complete dataset\n",
    "suffix = '_FULL' if not sample else ''\n",
    "save_path = f'data/helix_hss/{target}_helix_data_{mina}_{maxa}{suffix}_{MODEL_NAME}.pt'\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('data/helix_hss', exist_ok=True)\n",
    "# Save tensors\n",
    "torch.save({\n",
    "    'a': a,\n",
    "    'b':b,\n",
    "    'a+b':a_b,\n",
    "    'hidden_states': hss\n",
    "}, save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run_hs('a', sample = False)\n",
    "#run_hs('a', sample = True)\n",
    "#get_nums_hss('a', sample = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nums_hss(target, mina = 0,maxa = 99, sample = False):\n",
    "    suffix = '_FULL' if not sample else ''\n",
    "    save_path = f'data/helix_hss/{target}_helix_data_{mina}_{maxa}{suffix}_{MODEL_NAME}.pt'\n",
    "    obj = torch.load(save_path, weights_only=True)\n",
    "    a,b,a_b, hss = obj['a'], obj['b'], obj['a+b'], obj['hidden_states']\n",
    "    return a,b,a_b, hss\n",
    "# hss is a tensor containing hidden states from each layer of the model\n",
    "# Shape: [batch_size, num_layers, hidden_dim]\n",
    "# - batch_size: number of examples in the dataset\n",
    "# - num_layers: number of transformer layers (28 for GPT-J)\n",
    "# - hidden_dim: dimension of hidden states (4096 for GPT-J)\n",
    "# Hidden states are captured at the answer token position for each arithmetic problem\n",
    "a,b,a_b, hss = get_nums_hss('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9), tensor(51), tensor(60), torch.Size([28, 4096]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0], b[0], a_b[0], hss[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0076,  0.0048, -0.0003,  ..., -0.0287,  0.0079,  0.0200],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hss[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500]),\n",
       " torch.Size([500]),\n",
       " torch.Size([500]),\n",
       " torch.Size([500, 28, 4096]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape, a_b.shape, hss.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Functions\n",
    "\n",
    "\n",
    "This section performs Principal Component Analysis (PCA) on the hidden states of the numbers.\n",
    "\n",
    "The functions include:\n",
    "1. run_pca: Performs PCA on the hidden states for a given target variable\n",
    "2. get_pca: Retrieves the PCA components for a specific layer and target variable\n",
    "3. get_pca_data: Retrieves the PCA data for a specific layer and target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:21<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_pca(target, mina = 0, maxa = 99, NUM_PCA = 100):\n",
    "    _,_,_, hss = get_nums_hss(target, mina, maxa) # we want the full thing for a pca\n",
    "    pca_components = {}\n",
    "    for layer in tqdm(range(hss.shape[1])):\n",
    "        hs = hss[:,layer]\n",
    "        # Convert to numpy for sklearn\n",
    "        hs_numpy = hs.float().numpy()\n",
    "        # Fit PCA without dimensionality reduction\n",
    "        pca = PCA(n_components=NUM_PCA)# None\n",
    "        pca.fit(hs_numpy)\n",
    "        # Store components for this layer\n",
    "        pca_components[layer] = {\n",
    "            'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "            'components': pca.components_,\n",
    "            'singular_values': pca.singular_values_,\n",
    "            'mean': pca.mean_\n",
    "        }\n",
    "    # Save PCA components to file with descriptive name\n",
    "    save_path = f'data/helix_pca/{target}_pca_data_{mina}_{maxa}_{MODEL_NAME}.pt'\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs('data/helix_pca', exist_ok=True)\n",
    "    # Save dictionary\n",
    "    torch.save(pca_components, save_path)\n",
    "    return pca_components\n",
    "\n",
    "def get_pca(target, layer, mina = 0, maxa = 99):\n",
    "    save_path = f'data/helix_pca/{target}_pca_data_{mina}_{maxa}_{MODEL_NAME}.pt'\n",
    "    pca_data = torch.load(save_path, weights_only = False)[layer]\n",
    "    return pca_data\n",
    "\n",
    "run_pca('b')\n",
    "pca_data = get_pca('b', 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Numerical Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated hidden states for 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 406.25x142.188 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "COL_WIDTH = 3.25\n",
    "TWO_COL_WIDTH = 6.75\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,               # Default text size\n",
    "    'axes.titlesize': 7,          # Title size for axes\n",
    "    'axes.labelsize': 7,          # Axis label size\n",
    "    'xtick.labelsize': 6,         # X-axis tick label size\n",
    "    'ytick.labelsize': 6,         # Y-axis tick label size\n",
    "    'legend.fontsize': 6,         # Legend font size\n",
    "    'figure.titlesize': 10,        # Overall figure title size\n",
    "})\n",
    "def get_nums_hidden_states(mina = 0,maxa = 99):\n",
    "    nums = np.linspace(mina, maxa, maxa-mina+1)\n",
    "    tokens = torch.tensor(model.tokenizer([f'{int(num)}' for num in nums])['input_ids'])\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        with model.trace(validate=False,remote=remote) as tracer:\n",
    "            with tracer.invoke(tokens, scan=False):\n",
    "                for layer in range(NLAYERS):\n",
    "                    activations.append(model.transformer.h[layer].inputs[1]['hidden_states'][:,-1].save())\n",
    "    act = torch.stack(activations).detach().cpu()\n",
    "    print(f'Generated hidden states for {len(nums)}')\n",
    "    return nums, act\n",
    "\n",
    "\n",
    "\n",
    "def get_pc1(mina, maxa, skip0, layer):\n",
    "    nums, hss = get_nums_hidden_states(mina, maxa)\n",
    "    hs = hss[layer].cpu().float().numpy()\n",
    "    # Perform PCA\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    hs_scaled = scaler.fit_transform(hs)\n",
    "    \n",
    "    # Get first principal component\n",
    "    pca = PCA(n_components=1)\n",
    "    pc1_vals = pca.fit_transform(hs_scaled).flatten()\n",
    "    \n",
    "    # Print variance explained by PC1\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(1.25*COL_WIDTH, 1.25*0.35*COL_WIDTH))\n",
    "    \n",
    "    # Create scatter plot colored by number value\n",
    "    scatter = plt.scatter(nums, pc1_vals, c=nums, cmap='viridis', label = 'PC1')\n",
    "    \n",
    "    # Calculate line of best fit\n",
    "    slope, intercept = np.polyfit(nums, pc1_vals, 1)\n",
    "    line_x = nums\n",
    "    line_y = slope * line_x + intercept\n",
    "    \n",
    "    # Calculate R^2\n",
    "    r_squared = np.corrcoef(nums, pc1_vals)[0,1]**2\n",
    "    \n",
    "    # Plot line of best fit\n",
    "    plt.plot(line_x, line_y, 'r--', label=f'Best fit (R² = {r_squared:.3f})')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    plt.xlabel('Number')\n",
    "    plt.ylabel('PC1 Value')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    # Create directory if it doesn't exist\n",
    "    # Save figure with informative filename\n",
    "    # plt.savefig(f'paper_figures/figure_2/pc1_layer{layer-1}_range{mina}-{maxa}_skip0={skip0}_{MODEL_NAME}.pdf', \n",
    "    #             bbox_inches='tight', dpi = 300)\n",
    "    plt.show()\n",
    "\n",
    "get_pc1(mina = 0, maxa = 99, skip0 = False, layer = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Interpretation\n",
    "\n",
    "PC1 captures **the strongest numerical trend** in how numbers are represented.\n",
    "\n",
    "If numbers were linearly embedded, PC1 would strongly correlate with numerical magnitude.\n",
    "\n",
    "However, nonlinear patterns (e.g., periodic structures) appear in higher components, hinting at a helical representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated hidden states for 100\n"
     ]
    }
   ],
   "source": [
    "mina, maxa = 0, 99\n",
    "layer = 1\n",
    "nums, hss = get_nums_hidden_states(mina, maxa)\n",
    "hs = hss[layer].cpu().float().numpy()\n",
    "# Perform PCA\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "hs_scaled = scaler.fit_transform(hs)\n",
    "\n",
    "# Get first principal component\n",
    "pca = PCA(n_components=1)\n",
    "pc1_vals = pca.fit_transform(hs_scaled).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), torch.Size([28, 100, 4096]), (100,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.shape, hss.shape, pc1_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(-18.110722)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc1_vals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transform\n",
    "\n",
    "We analyze the frequency components of the hidden state representations using Fourier transforms. \n",
    "\n",
    "By decomposing the hidden states into their constituent frequencies, we can identify any periodic patterns or oscillations in how the model represents numbers. \n",
    "\n",
    "This analysis helps reveal the underlying structure of the model's numerical representations and complements our earlier PCA findings about potential helical patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 406.25x132.031 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'font.size': 8,               # Default text size\n",
    "    'axes.titlesize': 7,          # Title size for axes\n",
    "    'axes.labelsize': 7,          # Axis label size\n",
    "    'xtick.labelsize': 6,         # X-axis tick label size\n",
    "    'ytick.labelsize': 6,         # Y-axis tick label size\n",
    "    'legend.fontsize': 7,         # Legend font size\n",
    "    'figure.titlesize': 10,        # Overall figure title size\n",
    "})\n",
    "def get_nums_hss(mina = 0,maxa = 99):\n",
    "    nums = np.linspace(mina, maxa, maxa-mina+1)\n",
    "    tokens = torch.tensor(model.tokenizer([f'{int(num)}' for num in nums])['input_ids'])\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        with model.trace(validate=False,remote=remote) as tracer:\n",
    "            with tracer.invoke(tokens, scan=False):\n",
    "                for layer in range(NLAYERS):\n",
    "                    activations.append(model.transformer.h[layer].inputs[1]['hidden_states'][:,-1].save())\n",
    "    act = torch.stack(activations).detach().cpu()\n",
    "    return nums, act\n",
    "\n",
    "def ff(mina = 0, maxa = 500, layer = 0):\n",
    "    \n",
    "    nums, hss = get_nums_hss(mina, maxa)\n",
    "    hss_np = hss[layer].float().cpu().numpy()\n",
    "    hss_centered = hss_np# - np.mean(hss_np, axis=0, keepdims=True)\n",
    "    # Apply Hamming window\n",
    "    window = np.hamming(hss_np.shape[0])[:, np.newaxis]\n",
    "    fft_result = fft(hss_centered, axis=0)\n",
    "    # Get frequencies\n",
    "    n_samples = hss_np.shape[0]\n",
    "    freqs = fftfreq(n_samples, d=1)  # d=1 since numbers are 1 unit apart\n",
    "    # Calculate magnitude spectrum\n",
    "    magnitude_spectrum = np.abs(fft_result)**2\n",
    "    # Average across hidden dimensions to get overall frequency importance\n",
    "    avg_magnitude = np.mean(magnitude_spectrum, axis=1)\n",
    "    # Plot only positive frequencies (first half)\n",
    "    positive_freq_mask = freqs > 0\n",
    "    plt.figure(figsize=(1.25*COL_WIDTH, 1.25*0.325*COL_WIDTH))\n",
    "    plt.plot(freqs[positive_freq_mask], avg_magnitude[positive_freq_mask],label = f'Fourier Decomposition', color = 'b')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Magnitude')\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    \n",
    "    # Label specific periods\n",
    "    periods_to_label = [2, 5, 10, 100]\n",
    "    for period in periods_to_label:\n",
    "        freq = 1/period\n",
    "        # Find the closest frequency in our data\n",
    "        idx = np.argmin(np.abs(freqs[positive_freq_mask] - freq))\n",
    "        mag = avg_magnitude[positive_freq_mask][idx]\n",
    "        plt.annotate(f'T={period}', \n",
    "                    xy=(freq, mag),\n",
    "                    xytext=(4, 4),\n",
    "                    textcoords='offset points',\n",
    "                    ha='left',\n",
    "                    va='bottom',\n",
    "                    bbox=dict(boxstyle='round,pad=0.1', fc='yellow', alpha=0.5),\n",
    "                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'paper_figures/figure_2/fourier_{MODEL_NAME}.pdf', dpi =300, bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "ff(mina = 0, maxa = 360, layer = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the Fourier transform to the hidden state representations of numbers in GPT-J’s residual stream. \n",
    "\n",
    "The analysis reveals that these embeddings exhibit **strong periodicity**, with dominant frequency components at `T = [2, 5, 10, 100]`. These periodic structures suggest that LLMs represent numbers not just as linear embeddings but also as cyclical patterns — a key characteristic of a generalized helix. The presence of a T = 10 component aligns with the base-10 structure of numbers, while T = 2 likely captures even-odd distinctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting A Helical Representation\n",
    "\n",
    "### Mathematical Definition of a Helix\n",
    "\n",
    "**1. Standard 3D Helix**\n",
    "\n",
    "A helix in three-dimensional space with **radius** $ r $ and **pitch** $ p $ is defined as:\n",
    "\n",
    "$x = r \\cos(t), y = r \\sin(t), z = \\frac{p}{2\\pi} t$\n",
    "\n",
    "where:\n",
    "- $ t $ is a continuous parameter (often representing time or angle in radians),\n",
    "- $ r $ is the radius of the helix,\n",
    "- $ p $ is the pitch, which determines how far the helix moves along the z-axis per revolution.\n",
    "\n",
    "**2. Generalized Helix in LLMs**\n",
    "\n",
    "The paper suggests that **LLMs represent numbers as a generalized helix**, incorporating both linear and periodic components.\n",
    "\n",
    "A number $ a $ is embedded as:\n",
    "\n",
    "$h(a) = C B(a)^T$\n",
    "\n",
    "where:\n",
    "\n",
    "$\n",
    "B(a) =\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "\\cos\\left(\\frac{2\\pi}{T_1} a\\right) \\\\\n",
    "\\sin\\left(\\frac{2\\pi}{T_1} a\\right) \\\\\n",
    "\\cos\\left(\\frac{2\\pi}{T_2} a\\right) \\\\\n",
    "\\sin\\left(\\frac{2\\pi}{T_2} a\\right) \\\\\n",
    "\\vdots \\\\\n",
    "\\cos\\left(\\frac{2\\pi}{T_k} a\\right) \\\\\n",
    "\\sin\\left(\\frac{2\\pi}{T_k} a\\right)\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "- $ T_1, T_2, \\dots, T_k $ are the periodic components (e.g., 2, 5, 10, 100 from the paper),\n",
    "- $ C $ is a transformation matrix that maps the **basis functions** into the model’s latent space.\n",
    "\n",
    "**Matrix Representation**\n",
    "\n",
    "If we consider a set of numbers $ \\{a_1, a_2, \\dots, a_n\\} $, their helical representation can be written as:\n",
    "\n",
    "$\n",
    "H =\n",
    "\\begin{bmatrix}\n",
    "a_1 & \\cos\\left(\\frac{2\\pi}{T_1} a_1\\right) & \\sin\\left(\\frac{2\\pi}{T_1} a_1\\right) & \\dots & \\cos\\left(\\frac{2\\pi}{T_k} a_1\\right) & \\sin\\left(\\frac{2\\pi}{T_k} a_1\\right) \\\\\n",
    "a_2 & \\cos\\left(\\frac{2\\pi}{T_1} a_2\\right) & \\sin\\left(\\frac{2\\pi}{T_1} a_2\\right) & \\dots & \\cos\\left(\\frac{2\\pi}{T_k} a_2\\right) & \\sin\\left(\\frac{2\\pi}{T_k} a_2\\right) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\dots & \\vdots & \\vdots \\\\\n",
    "a_n & \\cos\\left(\\frac{2\\pi}{T_1} a_n\\right) & \\sin\\left(\\frac{2\\pi}{T_1} a_n\\right) & \\dots & \\cos\\left(\\frac{2\\pi}{T_k} a_n\\right) & \\sin\\left(\\frac{2\\pi}{T_k} a_n\\right)\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Each row represents the **helical embedding** of a number $ a_i $, capturing both its **linear and periodic** properties.\n",
    "\n",
    "---\n",
    "**Interpretation**\n",
    "- The first column represents the **linear** nature of the number.\n",
    "- The remaining columns encode periodic information with different frequencies.\n",
    "- The matrix $ C $ applies a transformation to project these representations into a latent space useful for LLMs.\n",
    "\n",
    "This helical representation allows LLMs to efficiently **manipulate numbers** using trigonometric transformations rather than relying solely on a linear encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(fit_type, nums, bases = [2,5,10,100], polybase = 9):\n",
    "    # creates the basis of features of helix regression\n",
    "    assert fit_type in ['helix', 'poly', 'circle']\n",
    "    feature_list = []\n",
    "    for i, num in enumerate(nums):\n",
    "        if fit_type == 'helix' or fit_type == 'circle':\n",
    "            for base in bases:\n",
    "                t_scaled = 2 * np.pi * num / base\n",
    "                feature_list.extend([np.cos(t_scaled), np.sin(t_scaled)])\n",
    "        if fit_type == 'helix':\n",
    "            feature_list.append(num / max(num))  # Add linear term for helix\n",
    "        if fit_type == 'poly':\n",
    "            feature_list = [num**power for power in range(1,polybase + 1)]\n",
    "    feature_list.append(np.ones(nums.shape[1])) # bias term\n",
    "    helix_features = np.column_stack(feature_list)\n",
    "    return helix_features\n",
    "\n",
    "def get_pca(target, layer, mina = 0, maxa = 99):\n",
    "    save_path = f'data/helix_pca/{target}_pca_data_{mina}_{maxa}_{MODEL_NAME}.pt'\n",
    "    pca_data = torch.load(save_path, weights_only = False)[layer]\n",
    "    return pca_data\n",
    "\n",
    "def get_pca_coords(target, layer, mina = 0, maxa = 99, npca = False, reconstruction = False):\n",
    "    assert isinstance(npca, bool) or isinstance(npca, int), \"npca must be either False or an integer\"\n",
    "    a,b,a_b, hss = get_nums_hss(target, mina, maxa, sample = True) # just take subset\n",
    "    hss = hss[:,layer]\n",
    "    pca_data = get_pca(target, layer, mina, maxa)\n",
    "    pca_comps, pca_mean = pca_data['components'], pca_data['mean']\n",
    "    #print(hss.shape, pca_comps.shape)\n",
    "    #assert hss.shape[0] == pca_comps.shape[0] # same batch\n",
    "    # Project data into PCA space\n",
    "    hs_numpy = hss.float().numpy()\n",
    "    hs_centered = hs_numpy - pca_mean\n",
    "    pca_coords = np.dot(hs_centered, pca_comps.T)\n",
    "    if npca is False:\n",
    "        npca = hss.shape[0] # make it full batch\n",
    "    pca_coords = pca_coords[:, :npca]\n",
    "    if reconstruction:\n",
    "        reconstruction = torch.tensor(np.dot(pca_coords, pca_comps[:npca]) + pca_mean)\n",
    "        mse = torch.mean((reconstruction - hss) ** 2)\n",
    "        #print(f\"MSE between reconstruction and original hidden states: {mse:.6f}\")\n",
    "        return a,b,a_b, reconstruction\n",
    "    else:\n",
    "        return a,b,a_b, pca_coords\n",
    "\n",
    "s = get_pca_coords('a', 5, npca = 3, reconstruction=True)\n",
    "s = get_pca_coords('b', 5, npca = 3, reconstruction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.04511994)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "\n",
    "def get_train_test_split(frac=0.75):\n",
    "    \"\"\"Returns which values of a (0-99) are in train vs test sets\"\"\"\n",
    "    a_values = np.arange(100)\n",
    "    n_train = int(100 * frac)\n",
    "    train_a = np.random.choice(a_values, size=n_train, replace=False)\n",
    "    return train_a\n",
    "\n",
    "def get_train_mask(a, frac = 0.75):\n",
    "    train_a = get_train_test_split(frac)\n",
    "    \"\"\"Returns boolean mask indicating which elements of a are in training set\"\"\"\n",
    "    return np.isin(a, train_a)\n",
    "\n",
    "def helix_fit(layer, helix_type, bases=[2,5,10,100], polybase=9, save=True, traintest=False, random=False):\n",
    "    # target is whatever comes after helix_ in helix_type\n",
    "    # fitting the helix\n",
    "    fit_type, target = helix_type.split('_', 1)\n",
    "    a,b,a_b, hss_projected_n = get_pca_coords(target, layer, mina=0, maxa=99, npca=False, reconstruction=False)\n",
    "    hss_original = get_nums_hss(target, mina=0, maxa=99)[-1][:,layer]\n",
    "    pca_data = get_pca(target, layer, mina=0, maxa=99)\n",
    "    pca_comp, pca_mean = pca_data['components'], pca_data['mean']\n",
    "\n",
    "    if target == 'a_b':\n",
    "        nums = np.array([a,b])\n",
    "    elif target == 'a':\n",
    "        nums = np.array([a])\n",
    "    elif target == 'b':\n",
    "        nums = np.array([b])\n",
    "    helix_features = create_features(fit_type, nums, bases=bases, polybase=polybase)\n",
    "    \n",
    "    # Split data for train/test if requested\n",
    "    train_mask = None\n",
    "    if traintest:\n",
    "        train_mask = get_train_mask(a)\n",
    "        train_features = helix_features[train_mask]\n",
    "        train_targets = hss_projected_n[train_mask]\n",
    "    else:\n",
    "        train_features = helix_features\n",
    "        train_targets = hss_projected_n\n",
    "    \n",
    "    if random:\n",
    "        np.random.shuffle(train_features)\n",
    "    #print(train_features)\n",
    "        \n",
    "    # Fit a separate linear model for each reduced dimension\n",
    "    weights_by_dim = []\n",
    "    predictions = np.zeros_like(hss_projected_n)\n",
    "    models = []\n",
    "\n",
    "    # Suppress convergence warnings during model fitting\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "        for i in range(hss_projected_n.shape[1]):\n",
    "            model = LinearRegression(fit_intercept=False)\n",
    "            #model = Lasso(alpha=0.01, fit_intercept=False)\n",
    "            model.fit(train_features, train_targets[:, i])\n",
    "            models.append(model)\n",
    "            weights_by_dim.append(model.coef_)\n",
    "            predictions[:, i] = model.predict(helix_features)  # Predict on all data\n",
    "            \n",
    "    # Transform predictions back to original space using SVD components\n",
    "    predictions_full_space = np.dot(predictions, pca_comp) + pca_mean\n",
    "    # Calculate MSE in original space\n",
    "    mse = np.mean((predictions_full_space - hss_original.float().numpy()) ** 2)\n",
    "    A = np.stack(weights_by_dim, axis=-1)  # Stack weights to form the matrix A\n",
    "    A_map = A @ pca_comp # 28, 4096\n",
    "\n",
    "    if save:\n",
    "        save_dir = 'data/helix_fit/trained_fit'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        if fit_type == 'poly':\n",
    "            helix_type = f'{helix_type}_{polybase}'\n",
    "        else:\n",
    "            helix_type = f'{helix_type}_{bases}'\n",
    "        if random:\n",
    "            helix_type = f'{helix_type}_random'\n",
    "        os.makedirs(os.path.join(save_dir, helix_type), exist_ok=True)\n",
    "        save_path = f\"{save_dir}/{helix_type}/{helix_type}_fit_layer{layer}_{MODEL_NAME}{'_traintest' if traintest else ''}.pkl\"\n",
    "        with open(save_path, 'wb') as f:\n",
    "            save_dict = {\n",
    "                'a': a,\n",
    "                'b': b,\n",
    "                'a+b': a_b,\n",
    "                'predictions_full': predictions_full_space,\n",
    "                'A_map': A_map,\n",
    "                'pca_mean': pca_mean,\n",
    "            }\n",
    "            pickle.dump(save_dict, f)\n",
    "    return nums, predictions_full_space, mse, train_mask if traintest else None\n",
    "\n",
    "x = helix_fit(layer=1, helix_type='helix_a', random = True)\n",
    "#get_features([[1,2,3,4,5],[2,3,4,5,6], [3,5,7,9,11]], 'poly', [2,5,10,100]).shape\n",
    "x[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data'\n",
    "def get_helix(helix_type, layer):\n",
    "    save_dir = f'{datapath}/helix_fit/trained_fit'\n",
    "    save_path = f\"{save_dir}/{helix_type}/{helix_type}_fit_layer{layer}_{MODEL_NAME}.pkl\"\n",
    "    with open(save_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return torch.tensor(data['A_map'][:-1]).float(), get_feature_names()[:-1]\n",
    "\n",
    "def get_helix_ablation(helix_type, layer):\n",
    "    helix_abla, _ = get_helix(helix_type, layer)\n",
    "    helix_abla = np.linalg.pinv(helix_abla).T\n",
    "    return helix_abla # no need to ablate the bias direction\n",
    "\n",
    "def get_feature_names(bases = [2,5,10,100]):\n",
    "    variables = ['a']\n",
    "    feature_names = []\n",
    "    for i in range(len(variables)):\n",
    "        for base in bases:\n",
    "            feature_names.extend([f'cos({variables[i]}|T = {base})', f'sin({variables[i]}|T = {base})'])\n",
    "        feature_names.append(variables[i])\n",
    "    feature_names.append('1')\n",
    "    return feature_names # All features\n",
    "\n",
    "def get_helix_coords(layer):\n",
    "    a,_, _, hss = get_nums_hss(target = 'a')\n",
    "    hss = hss[:,layer].float()\n",
    "    # Create random hidden states with same shape as hss\n",
    "    #hss = torch.randn_like(hss)\n",
    "    feature_names = get_feature_names()\n",
    "    helix_comps = get_helix_ablation('helix_a_[2, 5, 10, 100]_random', layer)\n",
    "    # Get pseudo-inverse of helix components (9,4096) -> (4096,9)mponents\n",
    "    helix_coords = hss @ helix_comps.T # (100,9)\n",
    "    return a, helix_coords, feature_names\n",
    "\n",
    "\n",
    "def get_nums_hss(target, mina = 0,maxa = 99, sample = True):\n",
    "    suffix = '_FULL' if not sample else ''\n",
    "    save_path = f'{datapath}/helix_hss/{target}_helix_data_{mina}_{maxa}{suffix}_{MODEL_NAME}.pt'\n",
    "    obj = torch.load(save_path, weights_only=True)\n",
    "    a,b,a_b, hss = obj['a'], obj['b'], obj['a+b'], obj['hidden_states']\n",
    "    return a,b,a_b, hss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Formula for the Clock Algorithm**\n",
    "\n",
    "### **1. Helical Representation of a Number**\n",
    "A number $ a $ is encoded as a **generalized helix** in a high-dimensional space:\n",
    "\n",
    "$\n",
    "\\text{helix}(a) = \\left[ a, \\cos\\left(\\frac{2\\pi}{T_1} a\\right), \\sin\\left(\\frac{2\\pi}{T_1} a\\right), \\dots, \\cos\\left(\\frac{2\\pi}{T_k} a\\right), \\sin\\left(\\frac{2\\pi}{T_k} a\\right) \\right]\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ T_1, T_2, \\dots, T_k $ are periodic components (e.g., $ T = 2, 5, 10, 100 $).\n",
    "- This **combines linear and periodic encoding**, allowing structured numerical manipulation.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Addition in Helical Space**\n",
    "Instead of performing addition directly, the model **constructs the helix representation of $ a + b $** using trigonometric identities:\n",
    "\n",
    "$\n",
    "\\cos(a + b) = \\cos a \\cos b - \\sin a \\sin b\n",
    "$\n",
    "\n",
    "$\n",
    "\\sin(a + b) = \\sin a \\cos b + \\cos a \\sin b\n",
    "$\n",
    "\n",
    "Thus, the sum is computed as:\n",
    "\n",
    "$\n",
    "\\text{helix}(a + b) = F(\\text{helix}(a), \\text{helix}(b))\n",
    "$\n",
    "\n",
    "where $ F $ is a transformation function approximated by the **MLPs in the transformer**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Neural Network Processing**\n",
    "The **MLP layers** in the model approximate this sum via learned transformations:\n",
    "\n",
    "$\n",
    "h_{\\text{sum}} = f_{\\text{nonlinear}} \\left( W_{\\text{MLP}} \\cdot \\begin{bmatrix} \\cos a \\\\ \\sin a \\\\ \\cos b \\\\ \\sin b \\end{bmatrix} \\right)\n",
    "$\n",
    "\n",
    "which extracts features needed to compute $ \\text{helix}(a + b) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Final Output Projection**\n",
    "After constructing $ \\text{helix}(a+b) $, the **last MLP layer** maps this into output logits:\n",
    "\n",
    "$\n",
    "\\text{logits} = W_{\\text{output}} \\cdot h_{\\text{sum}}\n",
    "$\n",
    "\n",
    "where $ W_{\\text{output}} $ is a learned weight matrix projecting to the model's output space.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Final Formula for the Clock Algorithm**\n",
    "$\n",
    "\\text{helix}(a + b) = \\left[ a + b, \\cos\\left(\\frac{2\\pi}{T_1} (a + b)\\right), \\sin\\left(\\frac{2\\pi}{T_1} (a + b)\\right), \\dots \\right]\n",
    "$\n",
    "\n",
    "This is **constructed by attention heads and MLP layers** using the **Clock Algorithm** to compute structured periodic additions.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "The authors find that three mid-sized LLMs represent numbers as generalized helices and manipulate them using the interpretable Clock algorithm to compute addition. While LLMs could do addition linearly, we conjecture that LLMs use the Clock algorithm to improve accuracy, analogous to humans using decimal digits (which are a generalized helix with `T = [10, 100, . . . ]`) for addition rather than slide rules.\n",
    "\n",
    "\n",
    "\n",
    "When humans add numbers, we naturally break numbers into periodic components (digits).\n",
    "\n",
    "Instead of performing addition in a fully linear space (like using a slide rule, which would be a direct linear computation), we:\n",
    "1. Align numbers digit by digit (mod 10)\n",
    "2. Perform carries (mod 100, mod 1000, etc.)\n",
    "3. Use periodic structures to simplify calculations\n",
    "\n",
    "Thus, the decimal number system acts like a generalized helix with periodic cycles at `T = [10,100,1000,…]`, which makes addition more structured and accurate.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
