{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from nnsight import LanguageModel\n",
    "import plotly.graph_objects as go\n",
    "#from pyvene import BoundlessRotatedSpaceIntervention\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset as hf_Dataset\n",
    "from transformers import get_linear_schedule_with_warmup, AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from einops import einsum, repeat, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Just pass string instead of torch.device\n",
    "full_model_name = 'EleutherAI/gpt-j-6B'\n",
    "MODEL_NAME = full_model_name.split('/')[-1]\n",
    "model = LanguageModel(\n",
    "    full_model_name,\n",
    "    device_map=device,  # or you can use \"auto\" for automatic device mapping\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    dispatch=True,\n",
    "    trust_remote_code=True  # Add this if you're still getting warnings\n",
    ")\n",
    "remote = False\n",
    "NLAYERS = model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_df(mina = 0, maxa = 99):\n",
    "    df = pd.read_pickle(f'data_addition/gen_math/data_addition_correct_{mina}_{maxa}.pkl')\n",
    "    return df[df['correct'] == 1]\n",
    "\n",
    "def gen_intervention(samplesize = 1000, run = False, mina = 0, maxa = 99):\n",
    "    if run:\n",
    "        df = get_correct_df(mina = mina, maxa = maxa)\n",
    "        print(len(df))\n",
    "        # Drop the 'correct' column since we only have correct examples now\n",
    "        df = df.drop('correct', axis=1)\n",
    "        # Create cartesian product of df with itself\n",
    "        intervention_df = df.merge(df, how='cross', suffixes=('_original', '_intervened'))\n",
    "        # Filter out cases where original answer equals intervened answer\n",
    "        intervention_df = intervention_df[intervention_df['answer_original'] != intervention_df['answer_intervened']]\n",
    "        # Take random sample of size samplesize\n",
    "        intervention_df = intervention_df.sample(n=samplesize, random_state=42).reset_index(drop = True)\n",
    "        # Save to pickle file\n",
    "        intervention_df.to_pickle(f'data_addition/data_intervention_{mina}_{maxa}_sample{samplesize}.pkl')\n",
    "    intervention_df = pd.read_pickle(f'data_addition/data_intervention_{mina}_{maxa}_sample{samplesize}.pkl')\n",
    "    return intervention_df\n",
    "\n",
    "def get_sorted_neuron_df():\n",
    "   df = pd.read_csv('data_addition/neuron_att_patching.csv')\n",
    "   df = df.sort_values('logit_difference', ascending=False)\n",
    "   return df.reset_index(drop=True)\n",
    "\n",
    "def get_top_percentile_neurondf(fraction):\n",
    "   df = get_sorted_neuron_df()\n",
    "   n_rows = int(len(df) * fraction)\n",
    "   return df.head(n_rows).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#get_top_percentile_neurondf(0.1)\n",
    "df_int = gen_intervention(run = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1292)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_logit_diff(patched_logits, og_logits, cor_answer_tokens):\n",
    "    # Get logit differences between patched and original logits for each answer token\n",
    "    patched_target =  patched_logits[torch.arange(len(cor_answer_tokens)), cor_answer_tokens]\n",
    "    og_target = og_logits[torch.arange(len(cor_answer_tokens)), cor_answer_tokens]\n",
    "    logit_diffs = patched_target - og_target\n",
    "    return logit_diffs\n",
    "\n",
    "def log_diff_metrics(log_diff):\n",
    "    mean = log_diff.mean()\n",
    "    nonzero_mean = log_diff[log_diff != 0].mean() if (log_diff != 0).any() else torch.tensor(0.0)\n",
    "    return {'mean': mean, 'nonzero_mean': nonzero_mean}\n",
    "\n",
    "def path_patch_neuron(layer, neuron_idx, num_to_use = 100, batch_size=100):\n",
    "    df = gen_intervention(run = False)\n",
    "    if num_to_use is not None:\n",
    "        df = df.sample(n=num_to_use, random_state=42)\n",
    "    clean_tokens = torch.stack([torch.tensor(x) for x in df['q_tok_original'].values])\n",
    "    corrupt_tokens = torch.stack([torch.tensor(x) for x in df['q_tok_intervened'].values])\n",
    "    corrupt_answer_tokens = torch.stack([torch.tensor(x[0]) for x in df['answer_tok_intervened'].values])\n",
    "    with torch.no_grad():\n",
    "        all_log_diffs = []\n",
    "        for i in range(0, len(clean_tokens), batch_size):\n",
    "            batch_clean = clean_tokens[i:i+batch_size].to('cuda')\n",
    "            batch_corrupt = corrupt_tokens[i:i+batch_size].to('cuda') \n",
    "            batch_corrupt_answers = corrupt_answer_tokens[i:i+batch_size]\n",
    "            with model.trace() as tracer:\n",
    "                with tracer.invoke(batch_clean) as invoker_clean: # gets the clean tokens for calculation\n",
    "                    pass\n",
    "                with tracer.invoke(batch_corrupt) as invoker:\n",
    "                    neuron_val = model.transformer.h[layer].mlp.act.output[:,-1, neuron_idx].unsqueeze(1).save() #k, 1\n",
    "                    neuron_projection = model.transformer.h[layer].mlp.fc_out.weight[:, neuron_idx].unsqueeze(0)  #1, model_dim\n",
    "                    neuron_act_corrupt = (neuron_val * neuron_projection).save()\n",
    "                with tracer.invoke(batch_clean) as invoker:\n",
    "                    neuron_val = model.transformer.h[layer].mlp.act.output[:,-1, neuron_idx].unsqueeze(1).save() #k, 1\n",
    "                    neuron_projection = model.transformer.h[layer].mlp.fc_out.weight[:, neuron_idx].unsqueeze(0)  #1, model_dim\n",
    "                    neuron_act_clean = (neuron_val * neuron_projection).save()\n",
    "                    model.transformer.h[-1].output[0][:,-1] += neuron_act_corrupt - neuron_act_clean\n",
    "            \n",
    "                #print(neuron_act.shape)\n",
    "                output = model.output.save()\n",
    "            clean_logits, patched_logits = output.logits[:batch_size,-1].cpu(), output.logits[-batch_size:,-1].cpu()\n",
    "            log_diff = calc_logit_diff(patched_logits, clean_logits, batch_corrupt_answers).cpu()\n",
    "            all_log_diffs.append(log_diff)\n",
    "            # correct = calc_correct(patched_logits, cor_answer_tokens)\n",
    "            # metrics = {'logit_diff':log_diff, 'accuracy': correct}\n",
    "        logdiffs = torch.cat(all_log_diffs)\n",
    "        return logdiffs\n",
    "\n",
    "\n",
    "path_patch_neuron(20,7741, 300, 300).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4587/4587 [2:00:35<00:00,  1.58s/it, rank=4586, log_diff=0.0010, path_patch=-0.0002]\n"
     ]
    }
   ],
   "source": [
    "def patch_patch_all(frac = 0.01):\n",
    "    ndf = get_sorted_neuron_df()\n",
    "    ndf['path_patch'] = float('nan')\n",
    "    ndf['path_patch_nonzero'] = float('nan')\n",
    "    # Take top 1% by logit_difference\n",
    "    ndf = ndf.nlargest(int(len(ndf) * frac), 'logit_difference')\n",
    "    bar = tqdm(ndf.iterrows(), total=len(ndf))\n",
    "    for i, row in bar:\n",
    "        layer, neuron_idx = int(row['layer']), int(row['neuron_idx'])\n",
    "        log_diff = path_patch_neuron(layer, neuron_idx, num_to_use=300, batch_size=300)\n",
    "        metrics = log_diff_metrics(log_diff)\n",
    "        ndf.loc[i, 'path_patch'] = metrics['mean'].item()\n",
    "        ndf.loc[i, 'path_patch_nonzero'] = metrics['nonzero_mean'].item()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            ndf.to_csv('data_addition/neuron_path_patching.csv', index=False)\n",
    "        bar.set_postfix({'rank':i, 'log_diff':f\"{row['logit_difference']:.4f}\", 'path_patch':f\"{ndf.loc[i, 'path_patch']:.4f}\"})\n",
    "    # Save final results\n",
    "    ndf.to_csv('data_addition/neuron_path_patching.csv')\n",
    "\n",
    "#patch_patch_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "helix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
